{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNL/96KLlcZlkhVEi1Mkd1e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raaj-4320/the-exam-paper-generator/blob/main/topic_based_question_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bncFDCqzUOGA",
        "outputId": "023a7796-cb6a-475a-a973-ac8dd300dda3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.35.14-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.35.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "id": "hSCQAAVlHPxM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic = \"Matrix\""
      ],
      "metadata": {
        "id": "MLtq6BAik248"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_wikipedia(topic):\n",
        "    search_url = f\"https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch={topic}&format=json\"\n",
        "    response = requests.get(search_url)\n",
        "    search_results = response.json()\n",
        "    return search_results['query']['search']\n",
        "\n",
        "def clean_snippet(snippet):\n",
        "    soup = BeautifulSoup(snippet, \"html.parser\")\n",
        "    text = soup.get_text()\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    # Remove stop words and non-alphabetic tokens\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Example usage\n",
        "topic = \"Regression in machine learning\"\n",
        "search_results = search_wikipedia(topic)\n",
        "\n",
        "# Extract titles from the search results\n",
        "titles = [result['title'] for result in search_results]\n",
        "\n",
        "# Print related topics (titles)\n",
        "print(\"Related topics:\")\n",
        "for title in titles:\n",
        "    print(title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWJ-4Q16ZJMZ",
        "outputId": "7ab20b0c-f121-49db-b30a-16d764709912"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Related topics:\n",
            "Regression analysis\n",
            "Decision tree learning\n",
            "Support vector machine\n",
            "Feature (machine learning)\n",
            "Machine learning\n",
            "Outline of machine learning\n",
            "Supervised learning\n",
            "Statistical learning theory\n",
            "Hyperparameter (machine learning)\n",
            "Regularization (mathematics)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def get_wikipedia_extract(title, length=\"short\"):\n",
        "    # Define the endpoint\n",
        "    endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    # Set the extract length parameters\n",
        "    if length == \"short\":\n",
        "        exchars = 5000  # Limit to approximately one paragraph\n",
        "        params = {\n",
        "            \"action\": \"query\",\n",
        "            \"format\": \"json\",\n",
        "            \"titles\": title,\n",
        "            \"prop\": \"extracts\",\n",
        "            \"exchars\": exchars,\n",
        "            \"explaintext\": True\n",
        "        }\n",
        "    elif length == \"medium\":\n",
        "        exsentences = 20  # Limit to approximately two paragraphs\n",
        "        params = {\n",
        "            \"action\": \"query\",\n",
        "            \"format\": \"json\",\n",
        "            \"titles\": title,\n",
        "            \"prop\": \"extracts\",\n",
        "            \"exsentences\": exsentences,\n",
        "            \"explaintext\": True\n",
        "        }\n",
        "    else:\n",
        "        params = {\n",
        "            \"action\": \"query\",\n",
        "            \"format\": \"json\",\n",
        "            \"titles\": title,\n",
        "            \"prop\": \"extracts\",\n",
        "            \"exintro\": True,\n",
        "            \"explaintext\": True\n",
        "        }\n",
        "\n",
        "    # Make the request to the Wikipedia API\n",
        "    response = requests.get(endpoint, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Extract the page information\n",
        "    pages = data['query']['pages']\n",
        "    page = next(iter(pages.values()))  # Get the first (and only) page\n",
        "\n",
        "    # Extract the extract text\n",
        "    extract = page.get(\"extract\", \"No extract found.\")\n",
        "\n",
        "    return extract\n",
        "\n",
        "def search_wikipedia(topic):\n",
        "    search_url = f\"https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch={topic}&format=json\"\n",
        "    response = requests.get(search_url)\n",
        "    search_results = response.json()\n",
        "    return search_results['query']['search']\n",
        "\n",
        "# Example usage\n",
        "topic = \"Regression in machine learning\"\n",
        "search_results = search_wikipedia(topic)\n",
        "\n",
        "# Extract titles from the search results\n",
        "titles = [result['title'] for result in search_results]\n",
        "\n",
        "corpus = {}\n",
        "# Iterate through each title and get a paragraph (full extract) for each\n",
        "for title in titles:\n",
        "    extract = get_wikipedia_extract(title, length=\"full\")\n",
        "    print(f\"Title: {title}\\nExtract:\\n{extract}\\n\")\n",
        "    corpus[title] = extract\n",
        "    # the_text = (f\"Title: {title}\\nExtract:\\n{extract}\\n\")\n",
        "# print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zsha-gbq31_V",
        "outputId": "f5ac6b8b-a40e-4e40-e412-573f2d48b12b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Support vector machine\n",
            "Extract:\n",
            "In machine learning, support vector machines (SVMs, also support vector networks) are supervised max-margin models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most studied models, being based on statistical learning frameworks of VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). \n",
            "In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, which represent the data only through a set of pairwise similarity comparisons between the original data observations and representing the data by these transformed coordinates in the higher dimensional feature space. Thus, SVMs use the kernel trick to implicitly map their inputs into high-dimensional feature spaces. Being max-margin models, SVMs are resilient to noisy data (for example, mis-classified examples). SVMs can also be used for regression tasks, where the objective becomes \n",
            "  \n",
            "    \n",
            "      \n",
            "        ϵ\n",
            "      \n",
            "    \n",
            "    {\\displaystyle \\epsilon }\n",
            "  \n",
            "-sensitive.\n",
            "The support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data. These data sets require unsupervised learning approaches, which attempt to find natural clustering of the data to groups and, then, to map new data according to these clusters. \n",
            "The popularity of SVMs is likely due to their amenability to theoretical analysis, their flexibility in being applied to a wide variety of tasks, including structured prediction problems. It is not clear that SVMs have better predictive performance than other linear models, such as logistic regression and linear regression.\n",
            "\n",
            "Title: Regression analysis\n",
            "Extract:\n",
            "In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome' or 'response' variable, or a 'label' in machine learning parlance) and one or more independent variables (often called 'predictors', 'covariates', 'explanatory variables' or 'features'). The most common form of regression analysis is linear regression, in which one finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. For example, the method of ordinary least squares computes the unique line (or hyperplane) that minimizes the sum of squared differences between the true data and that line (or hyperplane). For specific mathematical reasons (see linear regression), this allows the researcher to estimate the conditional expectation (or population average value) of the dependent variable when the independent variables take on a given set of values. Less common forms of regression use slightly different procedures to estimate alternative location parameters (e.g., quantile regression or Necessary Condition Analysis) or estimate the conditional expectation across a broader collection of non-linear models (e.g., nonparametric regression).\n",
            "Regression analysis is primarily used for two conceptually distinct purposes. First, regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning. Second, in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables. Importantly, regressions by themselves only reveal relationships between a dependent variable and a collection of independent variables in a fixed dataset. To use regressions for prediction or to infer causal relationships, respectively, a researcher must carefully justify why existing relationships have predictive power for a new context or why a relationship between two variables has a causal interpretation. The latter is especially important when researchers hope to estimate causal relationships using  observational data.\n",
            "\n",
            "\n",
            "\n",
            "Title: Decision tree learning\n",
            "Extract:\n",
            "Decision tree learning is a supervised learning approach used in statistics, data mining and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations.\n",
            "Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. More generally, the concept of regression tree can be extended to any kind of object equipped with pairwise dissimilarities such as categorical sequences.\n",
            "Decision trees are among the most popular machine learning algorithms given their intelligibility and simplicity.\n",
            "In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data (but the resulting classification tree can be an input for decision making).\n",
            "\n",
            "Title: Feature (machine learning)\n",
            "Extract:\n",
            "In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon. Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition,  classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of \"feature\" is related to that of explanatory variable used in statistical techniques such as linear regression.\n",
            "\n",
            "\n",
            "\n",
            "Title: Machine learning\n",
            "Extract:\n",
            "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data and thus perform tasks without explicit instructions. Recently, artificial neural networks have been able to surpass many previous approaches in performance.\n",
            "ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. When applied to business problems, it is known under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.\n",
            "The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis (EDA) through unsupervised learning. \n",
            "From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.\n",
            "\n",
            "Title: Outline of machine learning\n",
            "Extract:\n",
            "The following outline is provided as an overview of and topical guide to machine learning:\n",
            "Machine learning – a subfield of soft computing within computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a \"field of study that gives computers the ability to learn without being explicitly programmed\". Machine learning involves the study and construction of algorithms that can learn from and make predictions on data. These algorithms operate by building a model from an example training set of input observations to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions.\n",
            "\n",
            "Title: Statistical learning theory\n",
            "Extract:\n",
            "Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis. Statistical learning theory deals with the statistical inference problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics.\n",
            "\n",
            "Title: Supervised learning\n",
            "Extract:\n",
            "Supervised learning (SL) is a paradigm in machine learning where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled supervisory signal) train a model. The training data is processed, building a function that maps new data on expected output values. An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.\n",
            "\n",
            "Title: Regularization (mathematics)\n",
            "Extract:\n",
            "In mathematics, statistics, finance, and computer science, particularly in machine learning and inverse problems, regularization is a process that changes the result answer to be \"simpler\". It is often used to obtain results for ill-posed problems or to prevent overfitting.\n",
            "Although regularization procedures can be divided in many ways, the following delineation is particularly helpful:\n",
            "\n",
            "Explicit regularization is regularization whenever one explicitly adds a term to the optimization problem.  These terms could be priors, penalties, or constraints. Explicit regularization is commonly employed with ill-posed optimization problems. The regularization term, or penalty, imposes a cost on the optimization function to make the optimal solution unique.\n",
            "Implicit regularization is all other forms of regularization.  This includes, for example, early stopping, using a robust loss function, and discarding outliers. Implicit regularization is essentially ubiquitous in modern machine learning approaches, including stochastic gradient descent for training deep neural networks, and ensemble methods (such as random forests and gradient boosted trees).\n",
            "In explicit regularization, independent of the problem or model, there is always a data term, that corresponds to a likelihood of the measurement and a regularization term that corresponds to a prior. By combining both using Bayesian statistics, one can compute a posterior, that includes both information sources and therefore stabilizes the estimation process. By trading off both objectives, one chooses to be more addictive to the data or to enforce generalization (to prevent overfitting). There is a whole research branch dealing with all possible regularizations. In practice, one usually tries a specific regularization and then figures out the probability density that corresponds to that regularization to justify the choice. It can also be physically motivated by common sense or intuition.\n",
            "In machine learning, the data term corresponds to the training data and the regularization is either the choice of the model or modifications to the algorithm. It is always intended to reduce the generalization error, i.e. the error score with the trained model on the evaluation set and not the training data.\n",
            "One of the earliest uses of regularization is Tikhonov regularization (ridge regression), related to the method of least squares.\n",
            "\n",
            "Title: Automated machine learning\n",
            "Extract:\n",
            "Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. It is the combination of automation and ML. \n",
            "AutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning. The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models. \n",
            "Common techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xaogGp2RioTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI"
      ],
      "metadata": {
        "id": "7qBEEb2oBRdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=''\n",
        ")"
      ],
      "metadata": {
        "id": "iz2qiUgMurQu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "37697f60-6170-471a-d1cf-cf91768c6e9f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'openai'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-da04ae764102>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m client = OpenAI(\n\u001b[1;32m      4\u001b[0m     \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "    model='gpt-3.5-turbo',\n",
        "    messages=[\n",
        "        {'role':'system','content':'you are a good teacher who sets a good exam papers'},\n",
        "        {'role': 'user','content':f'create 50 multiple choice questions of the given text {corpus} with easy complexity, and mention all the answer with the question itself marked as .'}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYOFTftuuwwf",
        "outputId": "39b19d35-5cb0-4afa-9225-72c572969967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionMessage(content='Sure, here are 50 easy multiple choice questions based on the given text:\\n\\n1. What is machine learning (ML)?\\na) A field of study in artificial intelligence\\nb) A method for explicitly programming computers\\nc) A type of hardware used in computer systems\\nd) A tool for manipulating data directly\\n\\n2. What is the primary focus of machine learning?\\na) Generating data from scratch\\nb) Learning from data and generalizing\\nc) Following explicit instructions only\\nd) Avoiding statistical algorithms\\n\\n3. What type of networks have surpassed many previous approaches in performance?\\na) Biological neural networks\\nb) Artificial neural networks\\nc) Quantum neural networks\\nd) Statistical neural networks\\n\\n4. In what fields does machine learning find application?\\na) Only in natural language processing\\nb) Only in computer vision\\nc) In various fields like medicine and agriculture\\nd) Solely in email filtering\\n\\n5. What is not a source of machine learning methods?\\na) Biological foundations\\nb) Computational statistics\\nc) Mathematical optimization\\nd) Data mining\\n\\n6. What is the related field of study focusing on exploratory data analysis through unsupervised learning?\\na) Mathematical programming\\nb) Data synthesis\\nc) Data mining\\nd) Predictive analytics\\n\\n7. What framework provides a basis for describing machine learning from a theoretical viewpoint?\\na) Perceptual approximation correct (PAC) learning\\nb) Partially accurate computation (PAC) learning\\nc) Probably approximately correct (PAC) learning\\nd) Probably accurate computation (PAC) learning\\n\\n8. What is quantum machine learning?\\na) Classical algorithms executed on a quantum computer\\nb) A field of study in classical machine learning\\nc) A method for improving computational speed\\nd) The integration of quantum algorithms within machine learning programs\\n\\n9. What type of methods does quantum machine learning utilize?\\na) Classical algorithms only\\nb) Qubits and quantum operations\\nc) Statistical algorithms\\nd) Mathematical optimization algorithms\\n\\n10. What does adversarial machine learning study?\\na) The defense against machine learning attacks\\nb) The efficiency of machine learning algorithms\\nc) The effectiveness of boosting algorithms\\nd) The relationship between classical and quantum machine learning\\n\\n11. What is boosting primarily used for?\\na) Reducing bias and variance\\nb) Generating random data\\nc) Increasing computational complexity\\nd) Modifying pre-existing algorithms\\n\\n12. What type of learner is a classifier that is only slightly correlated with the true classification?\\na) Strong\\nb) Weak\\nc) Neutral\\nd) Adaptive\\n\\n13. What is active learning in machine learning?\\na) A specialized learning algorithm for neural networks\\nb) A type of supervised learning involving human interaction\\nc) A technique for unsupervised data labeling\\nd) A process that requires passive data collection\\n\\n14. How is active learning different from traditional supervised learning?\\na) It involves consulting authoritative sources\\nb) It requires the user to query the algorithm\\nc) It involves the learner choosing examples to learn from\\nd) It is a single-pass learning process\\n\\n15. What is artificial intelligence (AI)?\\na) The study of biological intelligence\\nb) Intelligence exhibited by animals\\nc) Intelligence exhibited by machines\\nd) The study of computer hardware\\n\\n16. Who is considered one of the founding fathers of AI?\\na) Arthur Samuel\\nb) Robert Schapire\\nc) Alan Turing\\nd) John McCarthy\\n\\n17. What major event occurred in AI after deep learning surpassed all previous techniques?\\na) The development of adversarial machine learning\\nb) The creation of boost algorithms\\nc) The AI winter\\nd) The AI boom\\n\\n18. Which of the following is NOT an area where AI has made a significant impact?\\na) Robotics\\nb) Art\\nc) Astronomy\\nd) Strategy games\\n\\n19. What are the traditional goals of AI research?\\na) Reasoning, knowledge representation, and planning\\nb) Biology, physics, and chemistry\\nc) Psychology, mathematics, and economics\\nd) Computer hardware, software, and networking\\n\\n20. What type of systems does artificial intelligence seek to develop?\\na) Systems that can only perform one specific task\\nb) Systems that can perceive the environment and learn from it\\nc) Systems without any form of learning capabilities\\nd) Systems that require explicit instructions for all actions\\n\\n21. How is a neural network described in machine learning?\\na) A model inspired by biological neural networks\\nb) A quantum-based learning algorithm\\nc) A statistical approach to data analysis\\nd) A simplified version of boosting algorithms\\n\\n22. What does each artificial neuron in a neural network receive?\\na) Only input signals\\nb) Only output signals\\nc) Signals from connected neurons\\nd) Signals from random sources\\n\\n23. What are the connections between artificial neurons in a neural network called?\\na) Synapses\\nb) Messages\\nc) Signals\\nd) Nodes\\n\\n24. What determines the strength of the signal at each connection between neurons?\\na) Input data\\nb) The activation function\\nc) The output data\\nd) The network topology\\n\\n25. What is a deep neural network defined as?\\na) A network with no hidden layers\\nb) A network with exactly one hidden layer\\nc) A network with at least 3 hidden layers\\nd) A network with at least 2 hidden layers\\n\\n26. What tasks are artificial neural networks used for?\\na) Only image recognition\\nb) Only predictive modeling\\nc) A variety of tasks including adaptive control and predictive modeling\\nd) Tasks without requiring learning from experience\\n\\n27. What does the machine learning-based attention method simulate?\\na) Human intelligence\\nb) Machine learning performance\\nc) Human attention\\nd) Statistical algorithms\\n\\n28. How does the attention method determine the importance of each word in a sentence?\\na) By using hard weights\\nb) By calculating “soft” weights\\nc) By looking at the context window\\nd) By employing unsupervised learning\\n\\n29. What does the attention method allow for in terms of information access?\\na) Access to any part of a sentence directly\\nb) Access only through the previous hidden state\\nc) No access to hidden representations\\nd) Access only at the end of a sentence\\n\\n30. What is a transformer in the context of deep learning?\\na) A device for physical transformations\\nb) A device for machine translation only\\nc) A deep learning architecture based on the multi-head attention mechanism\\nd) A type of neural network with no activation function\\n\\n31. What are tokens in a transformer converted into at each layer?\\na) Hard weights\\nb) Binary values\\nc) Real numbers\\nd) Random vectors\\n\\n32. What paper introduced the transformer architecture?\\na) \"Attention Is All You Need\"\\nb) \"The Fast Weight Controller\"\\nc) \"Deep Learning for Dummies\"\\nd) \"Advanced Transformer Techniques\"\\n\\n33. What is the advantage of transformers compared to earlier architectures like LSTMs?\\na) They require more training time\\nb) They rely heavily on statistical methods\\nc) They have no recurrent units\\nd) They can only process text data\\n\\n34. Where is the transformer architecture used?\\na) Only in natural language processing\\nb) Only in computer vision\\nc) In various areas like audio and robotics\\nd) Solely in mathematical optimization\\n\\n35. What is adversarial machine learning the study of?\\na) The defense against machine learning attackers\\nb) The attacks on machine learning algorithms\\nc) Only evasion attacks\\nd) Byzantine attacks and model extraction\\n\\n36. What is boosting primarily used for in machine learning?\\na) Reducing variance only\\nb) Reducing bias and variance\\nc) Increasing computational complexity\\nd) Enhancing the interpretability of models\\n\\n37. What is active learning in machine learning?\\na) A type of reinforcement learning algorithm\\nb) A method for unsupervised data labeling\\nc) A scenario where the learner passively observes data\\nd) A learning algorithm that interacts with human input\\n\\n38. What are some situations where active learning is beneficial?\\na) When data labeling is cheap\\nb) When the learner selects uninformative examples\\nc) When the user is overwhelmed\\nd) When manual labeling is expensive\\n\\n39. What is the traditional goal of AI research?\\na) Developing highly specialized systems\\nb) Developing systems that can reason and learn\\nc) Creating systems without any learning capabilities\\nd) Focusing on hardware advancements only\\n\\n40. Who is considered a founding father of AI research?\\na) Alan Turing\\nb) John McCarthy\\nc) Robert Schapire\\nd) Marvin Minksy\\n\\n41. What significant event occurred in AI after the transformer architecture was introduced?\\na) The AI winter\\nb) The AI boom\\nc) The development of language models\\nd) The emergence of deep learning\\n\\n42. What aspects of life has AI influenced?\\na) Job markets only\\nb) All areas of life including healthcare and education\\nc) Solely economic sectors\\nd) Government policies only\\n\\n43. What are the subfields of AI research centered around?\\na) Specific goals and tools\\nb) General exploration\\nc) Animal intelligence\\nd) Quantum information\\n\\n44. What does artificial intelligence aim to develop?\\na) Systems without learning capabilities\\nb) Systems that rely purely on explicit instructions\\nc) Systems that can perceive and learn from the environment\\nd) Systems that are not influenced by outside data\\n\\n45. What are the connections between artificial neurons in a neural network called?\\na) Synapses\\nb) Nodes\\nc) Signals\\nd) Messages\\n\\n46. What determines the strength of the signal at each connection between neurons?\\na) Input data\\nb) The activation function\\nc) The output data\\nd) The network topology\\n\\n47. What is artificial neural network used for?\\na) Only image recognition\\nb) Multiple tasks like adaptive control and predictive modeling\\nc) Only text classification\\nd) Problems without complex data\\n\\n48. What method is used by attention to determine the importance of each word in a sentence?\\na) By using hard weights\\nb) By looking at the context window\\nc) By calculating “soft” weights\\nd) By employing supervised learning\\n\\n49. What is a transformer in the context of deep learning?\\na) A device for physical transformations\\nb) A deep learning architecture based on the multi-head attention mechanism\\nc) A type of neural network with no activation function\\nd) A device for machine translation only\\n\\n50. What paper introduced the transformer architecture?\\na) \"Attention Is All You Need\"\\nb) \"The Fast Weight Controller\"\\nc) \"Deep Learning for Dummies\"\\nd) \"Advanced Transformer Techniques\"', role='assistant', function_call=None, tool_calls=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U6pA_ItTuw5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NQ_3LmcKuw-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual Question generator"
      ],
      "metadata": {
        "id": "k0Suyw1zuxCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def convert_to_question(sentence):\n",
        "    # List of auxiliary verbs to identify questions\n",
        "    question_words = [\"is\", \"are\", \"am\", \"was\", \"were\", \"can\", \"could\", \"shall\", \"should\", \"will\", \"would\", \"do\", \"does\", \"did\", \"have\", \"has\", \"had\", \"may\", \"might\", \"must\"]\n",
        "\n",
        "    # Split the sentence into words\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Regex to identify verbs\n",
        "    verb_regex = re.compile(r\"\\bis\\b|\\bare\\b|\\bam\\b|\\bwas\\b|\\bwere\\b|\\bcan\\b|\\bcould\\b|\\bshall\\b|\\bshould\\b|\\bwill\\b|\\bwould\\b|\\bdo\\b|\\bdoes\\b|\\bdid\\b|\\bhave\\b|\\bhas\\b|\\bhad\\b|\\bmay\\b|\\bmight\\b|\\bmust\\b\")\n",
        "\n",
        "    # Find the first auxiliary verb in the sentence\n",
        "    for i, word in enumerate(words):\n",
        "        if verb_regex.match(word.lower()):\n",
        "            # Ensure we handle 'have' correctly\n",
        "            if word.lower() == \"have\" and i != 0:\n",
        "                question_sentence = \"Do \" + \" \".join(words[:i] + [words[i]] + words[i+1:]) + \"?\"\n",
        "            else:\n",
        "                # Move the auxiliary verb to the front\n",
        "                question_sentence = word + \" \" + \" \".join(words[:i] + words[i+1:]) + \"?\"\n",
        "            return question_sentence.capitalize()\n",
        "\n",
        "    # Handle sentences without auxiliary verbs by adding 'do'\n",
        "    if len(words) > 1:\n",
        "        return \"Do \" + sentence + \"?\"\n",
        "    else:\n",
        "        return sentence + \"?\"\n",
        "\n",
        "def paragraph_to_questions(paragraph):\n",
        "    # Split the paragraph into sentences\n",
        "    sentences = re.split(r'(?<=[.!?]) +', paragraph)\n",
        "    questions = [convert_to_question(sentence.strip()) for sentence in sentences if sentence.strip()]\n",
        "    return questions\n",
        "\n",
        "# Example input paragraph\n",
        "paragraph = \"\"\"\n",
        "I bought apples from market and bananas from super-market\n",
        "\"\"\"\n",
        "\n",
        "# Generate questions\n",
        "questions = paragraph_to_questions(paragraph)\n",
        "for question in questions:\n",
        "    print(question)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_VYImzx2Id2",
        "outputId": "39f54cdd-86b6-476d-e274-e99e3a3981ce"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do I bought apples from market and bananas from super-market?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "-Gs2x5hX8-yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New question generator"
      ],
      "metadata": {
        "id": "jMX4a9Aggcey"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iGuLihS1m1tV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hE3QNEC6m7lW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oes-HbLGm9l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2VZ0v3dKnCRL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yZDja5Q-nJxI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0naXbtzRpfF9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nsAVN5E9pnwl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rtP_lHmAqEKN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cKU1GQKvnSaq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NypsvEo-nWBt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}